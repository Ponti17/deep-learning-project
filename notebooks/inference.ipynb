{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import neptune\n",
    "from neptune_pytorch import NeptuneLogger\n",
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "import albumentations as A\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from dataloader.puma_dataset import PumaDataset\n",
    "from hover_net.dataloader.dataset import get_dataloader\n",
    "from hover_net.models import HoVerNetExt\n",
    "from hover_net.process import proc_valid_step_output, train_step, valid_step\n",
    "from hover_net.tools.utils import (dump_yaml, read_yaml,\n",
    "                                   update_accumulated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_yaml('../configs/consep_config.yaml')\n",
    "\n",
    "model = HoVerNetExt(\n",
    "    backbone_name=config[\"MODEL\"][\"BACKBONE\"],\n",
    "    pretrained_backbone=config[\"MODEL\"][\"PRETRAINED\"],\n",
    "    num_types=config[\"MODEL\"][\"NUM_TYPES\"]\n",
    ")\n",
    "model.load_state_dict(torch.load('../epoch_7.pth', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = '../data/01_training_dataset_tif_ROIs'\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.RandomCrop(width=256, height=256)\n",
    "])\n",
    "\n",
    "images = os.listdir(IMAGE_DIR)\n",
    "\n",
    "for image in images:\n",
    "    img = Image.open(os.path.join(IMAGE_DIR, image))\n",
    "    img = transform(image=np.array(img))['image']\n",
    "    img_tensor = torch.tensor(img).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    img_tensor = img_tensor[:, 1:, :, :]\n",
    "    print(img_tensor.shape)\n",
    "    out = model(img_tensor)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the output tensors\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "# Plot tp tensor\n",
    "axes[0].imshow(out['tp'].detach().numpy()[0, 0, :, :], cmap='viridis')\n",
    "axes[0].set_title('TP Output')\n",
    "\n",
    "# Plot np tensor\n",
    "axes[1].imshow(out['np'].detach().numpy()[0, 0, :, :], cmap='viridis')\n",
    "axes[1].set_title('NP Output')\n",
    "\n",
    "# Plot hv tensor\n",
    "axes[2].imshow(out['hv'].detach().numpy()[0, 0, :, :], cmap='viridis')\n",
    "axes[2].set_title('HV Output')\n",
    "\n",
    "axes[3].imshow(img, cmap='viridis')\n",
    "axes[3].set_title('Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hover_net.dataloader import get_dataloader\n",
    "from hover_net.postprocess import process\n",
    "from hover_net.process import infer_step, visualize_instances_dict\n",
    "from hover_net.tools.api import parse_single_instance\n",
    "\n",
    "def infer_one_image(\n",
    "    image_path,\n",
    "    model,\n",
    "    nr_types=3,\n",
    "    input_size=(512, 512),\n",
    "    device=\"cuda\",\n",
    "    show=False\n",
    "):\n",
    "    inference_dataloader = get_dataloader(\n",
    "        data_path=[image_path],\n",
    "        input_shape=input_size,\n",
    "        run_mode=\"inference_single\"\n",
    "    )\n",
    "\n",
    "    detection_list = []\n",
    "    segmentation_list = []\n",
    "    for step_idx, data in enumerate(inference_dataloader):\n",
    "        assert data.shape[0] == 1\n",
    "\n",
    "        test_result_output = infer_step(\n",
    "            batch_data=data, model=model, device=device\n",
    "        )\n",
    "        image_id = 0\n",
    "        for curr_image_idx in range(len(test_result_output)):\n",
    "            pred_inst, inst_info_dict = process(\n",
    "                test_result_output[curr_image_idx],\n",
    "                nr_types=nr_types,\n",
    "                return_centroids=True\n",
    "            )\n",
    "\n",
    "            for single_inst_info in inst_info_dict.values():\n",
    "                detection_dict, segmentation_dict = parse_single_instance(\n",
    "                    image_id, single_inst_info\n",
    "                )\n",
    "                detection_list.append(detection_dict)\n",
    "                segmentation_list.append(segmentation_dict)\n",
    "\n",
    "            if show:\n",
    "                src_image = data[0].numpy()\n",
    "                type_info_dict = {\n",
    "                    \"0\": [\"nolabe\", [0, 0, 0]],\n",
    "                    \"1\": [\"neopla\", [255, 0, 0]],\n",
    "                    \"2\": [\"inflam\", [0, 255, 0]],\n",
    "                    \"3\": [\"connec\", [0, 0, 255]],\n",
    "                    \"4\": [\"necros\", [255, 255, 0]],\n",
    "                    \"5\": [\"no-neo\", [255, 165, 0]]\n",
    "                }\n",
    "                type_info_dict = {\n",
    "                    int(k): (\n",
    "                        v[0], tuple(v[1])\n",
    "                    ) for k, v in type_info_dict.items()\n",
    "                }\n",
    "                overlay_kwargs = {\n",
    "                    \"draw_dot\": True,\n",
    "                    \"type_colour\": type_info_dict,\n",
    "                    \"line_thickness\": 2,\n",
    "                }\n",
    "                overlaid_img = visualize_instances_dict(\n",
    "                    src_image.copy(), inst_info_dict, **overlay_kwargs\n",
    "                )\n",
    "                plt.imshow(overlaid_img)\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "    return inst_info_dict, detection_list, segmentation_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = '../data/01_training_dataset_tif_ROIs'\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.RandomCrop(width=256, height=256)\n",
    "])\n",
    "\n",
    "images = os.listdir(IMAGE_DIR)\n",
    "\n",
    "for image in images:\n",
    "    img = Image.open(os.path.join(IMAGE_DIR, image))\n",
    "    img = transform(image=np.array(img))['image']\n",
    "    img_tensor = torch.tensor(img).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    img_tensor = img_tensor[:, 1:, :, :]\n",
    "    print(img_tensor.shape)\n",
    "    # out = model(img_tensor)\n",
    "    break\n",
    "\n",
    "\"\"\"\n",
    "    image_path,\n",
    "    model,\n",
    "    nr_types=3,\n",
    "    input_size=(512, 512),\n",
    "    device=\"cuda\",\n",
    "    show=False\n",
    "\"\"\"\n",
    "\n",
    "model.to('cuda')\n",
    "infer_one_image(os.path.join(IMAGE_DIR, images[0]), model, nr_types=4, input_size=(256, 256), show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-project-cTn929T1-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
