{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Misc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Augmentations\n",
    "import albumentations as A\n",
    "\n",
    "# Custom\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from hover_net.dataloader.dataset import get_dataloader\n",
    "from hover_net.datasets.puma_dataset import PumaDataset\n",
    "from hover_net.models import HoVerNetExt\n",
    "\n",
    "# Packages\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_fill_holes, measurements\n",
    "from skimage.segmentation import watershed\n",
    "\n",
    "# HoVer Net\n",
    "from hover_net.dataloader.preprocessing import get_bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __proc_np_hv(pred):\n",
    "    \"\"\"\n",
    "    Process Nuclei Prediction with XY Coordinate Map.\n",
    "\n",
    "    Args:\n",
    "        - pred: np.array(H, W, C)\n",
    "            C=0: nuclear pixel map,\n",
    "            C=1: horizontal map,\n",
    "            C=2: vertical map\n",
    "    Returns:\n",
    "        - proced_pred: np.array(H, W)\n",
    "            Numbered map of all nuclear instances.\n",
    "    \n",
    "    Source: https://github.com/vqdang/hover_net\n",
    "    \"\"\"\n",
    "    pred = np.array(pred, dtype=np.float32)\n",
    "\n",
    "    blb_raw = pred[..., 0]      # Probability map\n",
    "    h_dir_raw = pred[..., 1]    # x-map\n",
    "    v_dir_raw = pred[..., 2]    # y-map\n",
    "\n",
    "    # Processing\n",
    "    blb = np.array(blb_raw >= 0.5, dtype=np.int32)\n",
    "    blb = measurements.label(blb)[0]\n",
    "    blb[blb > 0] = 1\n",
    "\n",
    "    # Normalize direction maps\n",
    "    h_dir = cv2.normalize(\n",
    "        h_dir_raw,\n",
    "        None,\n",
    "        alpha=0,\n",
    "        beta=1,\n",
    "        norm_type=cv2.NORM_MINMAX,\n",
    "        dtype=cv2.CV_32F\n",
    "    )\n",
    "    v_dir = cv2.normalize(\n",
    "        v_dir_raw,\n",
    "        None,\n",
    "        alpha=0,\n",
    "        beta=1,\n",
    "        norm_type=cv2.NORM_MINMAX,\n",
    "        dtype=cv2.CV_32F\n",
    "    )\n",
    "\n",
    "    # Sobel calculates the derivaties of the image\n",
    "    # The derivatives will be high when there is a high change in intensity\n",
    "    # i.e. when going from one nuclei to another\n",
    "    # https://docs.opencv.org/4.x/d2/d2c/tutorial_sobel_derivatives.html\n",
    "    sobelh = cv2.Sobel(h_dir, cv2.CV_64F, 1, 0, ksize=21)\n",
    "    sobelv = cv2.Sobel(v_dir, cv2.CV_64F, 0, 1, ksize=21)\n",
    "\n",
    "    # Normalize the sobel maps\n",
    "    sobelh = 1 - (\n",
    "        cv2.normalize(\n",
    "            sobelh,\n",
    "            None,\n",
    "            alpha=0,\n",
    "            beta=1,\n",
    "            norm_type=cv2.NORM_MINMAX,\n",
    "            dtype=cv2.CV_32F\n",
    "        )\n",
    "    )\n",
    "    sobelv = 1 - (\n",
    "        cv2.normalize(\n",
    "            sobelv,\n",
    "            None,\n",
    "            alpha=0,\n",
    "            beta=1,\n",
    "            norm_type=cv2.NORM_MINMAX,\n",
    "            dtype=cv2.CV_32F\n",
    "        )\n",
    "    )\n",
    "\n",
    "    overall = np.maximum(sobelh, sobelv)\n",
    "    overall = overall - (1 - blb)\n",
    "    overall[overall < 0] = 0\n",
    "\n",
    "    dist = (1.0 - overall) * blb\n",
    "    # Nuclei values form mountains so inverse to get basins\n",
    "    dist = -cv2.GaussianBlur(dist, (3, 3), 0)\n",
    "\n",
    "    overall = np.array(overall >= 0.4, dtype=np.int32)\n",
    "\n",
    "    marker = blb - overall\n",
    "    marker[marker < 0] = 0\n",
    "    marker = binary_fill_holes(marker).astype(\"uint8\")\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    marker = cv2.morphologyEx(marker, cv2.MORPH_OPEN, kernel)\n",
    "    marker = measurements.label(marker)[0]\n",
    "\n",
    "    proced_pred = watershed(dist, markers=marker, mask=blb)\n",
    "\n",
    "    return proced_pred\n",
    "\n",
    "def process(pred_map):\n",
    "    \"\"\"\n",
    "    Post processing of the output of the HoVer-Net model.\n",
    "\n",
    "    Args:\n",
    "        - pred_map: np.array(H, W, C)\n",
    "            Combined output from all three branches of the HoVer-Net model.\n",
    "            C=0: type map,\n",
    "            C=1: nuclear pixel map,\n",
    "            C=2: horizontal map,\n",
    "            C=3: vertical map\n",
    "        - nr_types (int): number of types considered at output of nc branch\n",
    "\n",
    "    Returns:\n",
    "        - pred_inst:     pixel-wise nuclear instance prediction\n",
    "        - pred_type_out: dictionary containing instance information\n",
    "            bbox: bounding box of the instance\n",
    "            centroid: centroid of the instance\n",
    "            contour: contour of the instance\n",
    "            type_prob: probability of the instance belonging to a type\n",
    "            type: type of the instance\n",
    "    \n",
    "    Based on: https://github.com/vqdang/hover_net\n",
    "    \"\"\"\n",
    "    # Extract type and instance maps\n",
    "    # pred_type: np.array(H, W, 1)\n",
    "    # pred_inst: np.array(H, W, 3) => np, horizontal, vertical\n",
    "    pred_type = pred_map[..., :1]\n",
    "    pred_inst = pred_map[..., 1:]\n",
    "    pred_type = pred_type.astype(np.int32)\n",
    "\n",
    "    pred_inst = np.squeeze(pred_inst)\n",
    "    pred_inst = __proc_np_hv(pred_inst)\n",
    "\n",
    "    inst_info_dict = None\n",
    "    # Get unique instance ids w/o background\n",
    "    inst_id_list = np.unique(pred_inst)[1:]\n",
    "    inst_info_dict = {}\n",
    "\n",
    "    # Loop over each instance id\n",
    "    for inst_id in inst_id_list:\n",
    "        # Create map with only the current instance\n",
    "        inst_map = pred_inst == inst_id\n",
    "        inst_map = inst_map.astype(np.uint8)\n",
    "\n",
    "        # Get the countour of the instance\n",
    "        inst_contour = cv2.findContours(\n",
    "            inst_map, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "        inst_contour = np.squeeze(inst_contour[0][0].astype(\"int32\"))\n",
    "\n",
    "        # Skip a countour if it has less than 3 points\n",
    "        # Likely an artifact\n",
    "        if inst_contour.shape[0] < 3:\n",
    "            continue\n",
    "        if len(inst_contour.shape) != 2:\n",
    "            continue\n",
    "\n",
    "        # Get the moment of the nuclei instance\n",
    "        # Moment is the \"center of mass\" of the instance\n",
    "        # https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html\n",
    "        inst_moment = cv2.moments(inst_map)\n",
    "\n",
    "        # Create centroid of the instance from the moment\n",
    "        inst_centroid = [\n",
    "            (inst_moment[\"m10\"] / inst_moment[\"m00\"]),\n",
    "            (inst_moment[\"m01\"] / inst_moment[\"m00\"]),\n",
    "        ]\n",
    "\n",
    "        inst_centroid = np.array(inst_centroid)\n",
    "        inst_contour[:, 0] += inst_bbox[0][1]   # X\n",
    "        inst_contour[:, 1] += inst_bbox[0][0]   # Y\n",
    "        inst_centroid[0] += inst_bbox[0][1]     # X\n",
    "        inst_centroid[1] += inst_bbox[0][0]     # Y\n",
    "        inst_info_dict[inst_id] = {             # inst_id should start at 1\n",
    "            \"bbox\": inst_bbox,\n",
    "            \"centroid\": inst_centroid,\n",
    "            \"contour\": inst_contour,\n",
    "            \"type_prob\": None,\n",
    "            \"type\": None,\n",
    "        }\n",
    "\n",
    "    # * Get class of each instance id, stored at index id-1\n",
    "    for inst_id in list(inst_info_dict.keys()):\n",
    "        rmin, cmin, rmax, cmax = (\n",
    "            inst_info_dict[inst_id][\"bbox\"]\n",
    "        ).flatten()\n",
    "        inst_map_crop = pred_inst[rmin:rmax, cmin:cmax]\n",
    "        inst_type_crop = pred_type[rmin:rmax, cmin:cmax]\n",
    "        inst_map_crop = (\n",
    "            inst_map_crop == inst_id\n",
    "        )  # TODO: duplicated operation, may be expensive\n",
    "        inst_type = inst_type_crop[inst_map_crop]\n",
    "        type_list, type_pixels = np.unique(inst_type, return_counts=True)\n",
    "        type_list = list(zip(type_list, type_pixels))\n",
    "        type_list = sorted(type_list, key=lambda x: x[1], reverse=True)\n",
    "        inst_type = type_list[0][0]\n",
    "        if inst_type == 0:  # ! pick the 2nd most dominant if exist\n",
    "            if len(type_list) > 1:\n",
    "                inst_type = type_list[1][0]\n",
    "        type_dict = {v[0]: v[1] for v in type_list}\n",
    "        type_prob = type_dict[inst_type] / (np.sum(inst_map_crop) + 1.0e-6)\n",
    "        inst_info_dict[inst_id][\"type\"] = int(inst_type)\n",
    "        inst_info_dict[inst_id][\"type_prob\"] = float(type_prob)\n",
    "\n",
    "    return pred_inst, inst_info_dict\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "def infer_step(batch_imgs, model, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Infer a batch of images using the HoVer-net model.\n",
    "\n",
    "    Args:\n",
    "        - batch_imgs: torch.Tensor(B, H, W, C)\n",
    "        - model: torch.nn.Module\n",
    "    \n",
    "    Returns:\n",
    "        - pred_output: np.array(B, H, W, C)\n",
    "            C=0: type map,\n",
    "            C=1: nuclear pixel map,\n",
    "            C=2: horizontal map,\n",
    "            C=3: vertical map\n",
    "    \n",
    "    Based on: https://github.com/Kaminyou/HoVer-Net-PyTorch\n",
    "    \"\"\"\n",
    "    # Move images to gpu and permute to (B, C, H, W)\n",
    "    patch_imgs_gpu = batch_imgs.to(device).type(torch.float32)\n",
    "    patch_imgs_gpu = patch_imgs_gpu.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # ... And DONT compute gradients\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        # pred_dict: ordered dict with tp, np, hv keys\n",
    "        pred_dict = model(patch_imgs_gpu)\n",
    "\n",
    "        # Post-process the output\n",
    "        # Permute back to (B, H, W, C)\n",
    "        pred_list = []\n",
    "        for k, v in pred_dict.items():\n",
    "            pred_list.append([k, v.permute(0, 2, 3, 1).contiguous()])\n",
    "        pred_dict = OrderedDict(pred_list)\n",
    "\n",
    "        # Softmax the nuclear pixel map\n",
    "        pred_dict[\"np\"] = F.softmax(pred_dict[\"np\"], dim=-1)[..., 1:]\n",
    "\n",
    "        # Softmax the type map\n",
    "        type_map = F.softmax(pred_dict[\"tp\"], dim=-1)\n",
    "        type_map = torch.argmax(type_map, dim=-1, keepdim=True)\n",
    "        type_map = type_map.type(torch.float32)\n",
    "        pred_dict[\"tp\"] = type_map\n",
    "        pred_output = torch.cat(list(pred_dict.values()), -1)\n",
    "\n",
    "    return pred_output.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HoVerNetExt(\n",
    "    backbone_name=\"resnext\",\n",
    "    pretrained_backbone=True,\n",
    "    num_types=4\n",
    ")\n",
    "model.load_state_dict(torch.load('../pretrained/epoch_7.pth', weights_only=True))\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "IMAGE_PATH      = '../data/01_training_dataset_tif_ROIs'\n",
    "GEOJSON_PATH    = '../data/01_training_dataset_geojson_nuclei'\n",
    "PATCH_SIZE      = 256\n",
    "BATCH_SIZE      = 1\n",
    "\n",
    "# Load the dataset\n",
    "dataloader = get_dataloader(\n",
    "    image_path=IMAGE_PATH,\n",
    "    geojson_path=GEOJSON_PATH,\n",
    "    input_shape=(\n",
    "        PATCH_SIZE,\n",
    "        PATCH_SIZE\n",
    "    ),\n",
    "    mask_shape=(\n",
    "        PATCH_SIZE,\n",
    "        PATCH_SIZE\n",
    "    ),\n",
    "    run_mode=\"test\",\n",
    ")\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"Dataloader returns: {batch.keys()}\")\n",
    "\n",
    "pred = infer_step(\n",
    "            batch_imgs=batch['img'], model=model, device=\"cuda\"\n",
    "        )\n",
    "print(f\"Pred shape: {pred.shape}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(25, 5))\n",
    "ax[0].imshow(batch['img'][0])\n",
    "ax[1].imshow(pred[0, :, :, 0])\n",
    "ax[2].imshow(pred[0, :, :, 1])\n",
    "ax[3].imshow(pred[0, :, :, 2])\n",
    "ax[4].imshow(pred[0, :, :, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "pred_inst, inst_info_dict = process(\n",
    "    pred[0]\n",
    ")\n",
    "\n",
    "idx = 13\n",
    "print(f\"Pred inst shape: {pred_inst.shape}\")\n",
    "print(f\"inst_info_dict keys: {inst_info_dict.keys()}\")\n",
    "print(f\"inst_info_dict[1] keys: {inst_info_dict[idx].keys()}\")\n",
    "print(f\"inst_info_dict[1]['centroid']: {inst_info_dict[idx]['centroid']}\")\n",
    "print(f\"inst_info_dict[1]['type']: {inst_info_dict[idx]['type']}\")\n",
    "\n",
    "pred_inst = pred_inst > 0\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[2].imshow(pred_inst)\n",
    "\n",
    "\n",
    "rect = patches.Rectangle(\n",
    "    (inst_info_dict[idx][\"bbox\"][0][1], inst_info_dict[idx][\"bbox\"][0][0]),\n",
    "    inst_info_dict[idx][\"bbox\"][1][1] - inst_info_dict[idx][\"bbox\"][0][1],\n",
    "    inst_info_dict[idx][\"bbox\"][1][0] - inst_info_dict[idx][\"bbox\"][0][0],\n",
    "    linewidth=1,\n",
    "    facecolor=\"none\",\n",
    "    edgecolor=\"r\",\n",
    ")\n",
    "ax[2].add_patch(rect)\n",
    "ax[2].plot(\n",
    "    inst_info_dict[idx][\"contour\"][:, 0],  # x-coordinates\n",
    "    inst_info_dict[idx][\"contour\"][:, 1],  # y-coordinates\n",
    "    color='r'\n",
    ")\n",
    "\n",
    "ax[0].imshow(batch['img'][0])\n",
    "ax[1].imshow(batch['tp_map'][0])\n",
    "\n",
    "ax[1].set_title(\"Type Map GT\")\n",
    "ax[2].set_title(\"Nuclear Pixel Map Pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_type = pred[..., :1]\n",
    "pred_inst = pred[..., 1:]\n",
    "print(f\"pred_type shape: {pred_type.shape}\")\n",
    "print(f\"pred_inst shape: {pred_inst.shape}\")\n",
    "\n",
    "proc_pred = __proc_np_hv(np.squeeze(pred_inst))\n",
    "print(f\"proc_pred shape: {proc_pred.shape}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "ax[0].imshow(pred_inst[0, :, :, 0])\n",
    "ax[1].imshow(pred_inst[0, :, :, 1])\n",
    "ax[2].imshow(pred_inst[0, :, :, 2])\n",
    "ax[3].imshow(proc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_pred.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-project-cTn929T1-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
